import json
import base64
import asyncio
import io

from fastapi import APIRouter, WebSocket, WebSocketDisconnect
import websockets
from pydub import AudioSegment
from src.config import OPENAI_API_KEY

OPENAI_WS_URL = (
    "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01"
)

realtime_router = APIRouter()


def convert_webm_to_pcm16(webm_data: bytes) -> bytes:
    """Convert WebM/Opus audio to PCM16 24kHz mono for OpenAI."""
    try:
        print(f"ğŸ”„ Converting WebM audio ({len(webm_data)} bytes)")

        audio = AudioSegment.from_file(io.BytesIO(webm_data), format="webm")

        pcm_audio = audio.set_frame_rate(24000).set_channels(1).set_sample_width(2)

        print(f"âœ… Converted to PCM16: {len(pcm_audio.raw_data)} bytes")
        return pcm_audio.raw_data
    except Exception as e:
        print(f"ğŸ”´ Error converting audio: {e}")
        return b""


@realtime_router.websocket("/streamline")
async def streamline(websocket: WebSocket):
    print("ğŸ”Œ WebSocket connection request received")
    await websocket.accept()
    print("âœ… WebSocket connection accepted")

    openai_ws = None
    audio_buffer_size = 0
    response_active = False

    try:
        print("ğŸ”— Connecting to OpenAI WebSocket...")
        openai_ws = await websockets.connect(
            OPENAI_WS_URL,
            additional_headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "OpenAI-Beta": "realtime=v1",
            },
        )
        print("âœ… Connected to OpenAI WebSocket")

        print("ğŸ“ Sending session configuration...")
        session_config = {
            "type": "session.update",
            "session": {
                "modalities": ["audio", "text"],
                "voice": "alloy",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 500,
                    "create_response": True,
                },
                "instructions": (
                    "You are Langlish, a friendly and patient English learning "
                    "assistant. You help students improve their English through "
                    "conversation practice. Your role is to: help the user practice "
                    "english conversation, correct grammar mistakes gently, suggest "
                    "better vocabulary when appropriate, encourage the student, and "
                    "adapt to the student's level."
                ),
            },
        }
        await openai_ws.send(json.dumps(session_config))
        print("âœ… Session configuration sent")

        async def handle_client_messages():
            """Handle messages from the frontend client."""
            nonlocal audio_buffer_size, response_active

            try:
                while True:
                    message = await websocket.receive()

                    if "bytes" in message:
                        webm_data = message["bytes"]
                        audio_buffer_size += len(webm_data)
                        print(
                            f"ğŸ“¨ Received WebM audio: {len(webm_data)} bytes "
                            f"(total: {audio_buffer_size} bytes)"
                        )

                        pcm_data = convert_webm_to_pcm16(webm_data)
                        if pcm_data:
                            audio_b64 = base64.b64encode(pcm_data).decode("utf-8")

                            await openai_ws.send(
                                json.dumps(
                                    {
                                        "type": "input_audio_buffer.append",
                                        "audio": audio_b64,
                                    }
                                )
                            )
                            print(f"ğŸ“¤ Sent PCM16 audio to OpenAI ({len(pcm_data)} bytes)")
                        else:
                            print("ğŸ”´ Audio conversion failed, skipping chunk")
                    elif "text" in message:
                        try:
                            data = json.loads(message["text"])
                            if data.get("type") == "EOF" and not response_active:
                                print(
                                    f"ğŸ›‘ Received EOF with {audio_buffer_size} bytes "
                                    f"of total audio"
                                )

                                if audio_buffer_size > 5000:
                                    response_active = True
                                    await openai_ws.send(
                                        json.dumps({"type": "input_audio_buffer.commit"})
                                    )
                                    await openai_ws.send(
                                        json.dumps(
                                            {
                                                "type": "response.create",
                                                "response": {"modalities": ["audio", "text"]},
                                            }
                                        )
                                    )
                                    print("ğŸ“¤ Sent audio-based response request")
                                else:
                                    print("âš ï¸ Not enough audio data, skipping response")

                                audio_buffer_size = 0

                        except json.JSONDecodeError:
                            print(f"ğŸ“ Non-JSON text message: {message['text']}")

            except WebSocketDisconnect:
                print("ğŸ”´ Client disconnected")
            except Exception as e:
                print(f"ğŸ”´ Error handling client messages: {e}")

        async def handle_openai_responses():
            """Handle responses from OpenAI and send to frontend."""
            nonlocal response_active

            try:
                async for message in openai_ws:
                    event = json.loads(message)
                    event_type = event.get("type", "")

                    print(f"ğŸ“¨ OpenAI event: {event_type}")

                    if event_type == "response.audio.delta":
                        pcm_data = base64.b64decode(event["delta"])
                        await websocket.send_bytes(pcm_data)
                        print(f"ğŸµ Sent audio chunk to frontend ({len(pcm_data)} bytes)")

                    elif event_type == "response.text.delta":
                        text_delta = event.get("delta", "")
                        print(f"ğŸ“ Text response: {text_delta}")

                    elif event_type == "response.done":
                        print("âœ… Response completed")
                        response_active = False
                        await websocket.send_text("RESPONSE_COMPLETE")

                    elif event_type == "error":
                        error_msg = event.get("error", {}).get("message", "Unknown error")
                        print(f"ğŸ”´ OpenAI error: {error_msg}")
                        response_active = False
                        await websocket.send_text(f"ERROR: {error_msg}")

            except websockets.exceptions.ConnectionClosed:
                print("ğŸ”´ OpenAI WebSocket closed")
            except Exception as e:
                print(f"ğŸ”´ Error handling OpenAI responses: {e}")

        await asyncio.gather(
            handle_client_messages(),
            handle_openai_responses(),
            return_exceptions=True,
        )

    except Exception as e:
        print(f"ğŸ’¥ Error in streamline: {e}")
        try:
            await websocket.send_text(f"ERROR: {str(e)}")
        except Exception:
            pass
    finally:
        print("ğŸ§¹ Cleaning up...")
        if openai_ws:
            await openai_ws.close()
            print("âœ… OpenAI WebSocket closed")
        print("ğŸ Streamline session ended")